<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: weighting | Adam Ginsburg's Astro Blog]]></title>
  <link href="http://keflavich.github.com/blog/categories/weighting/atom.xml" rel="self"/>
  <link href="http://keflavich.github.com/"/>
  <updated>2012-12-28T17:22:06-07:00</updated>
  <id>http://keflavich.github.com/</id>
  <author>
    <name><![CDATA[Adam Ginsburg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Weighting and Scaling]]></title>
    <link href="http://keflavich.github.com/blog/2011/04/19/weighting-and-scaling/"/>
    <updated>2011-04-19T18:51:00-06:00</updated>
    <id>http://keflavich.github.com/blog/2011/04/19/weighting-and-scaling</id>
    <content type="html"><![CDATA[<div class='post'>
The "simple" relative sensitivity calibration was causing serious problems.<br /><br />The assumed model for a "gain" $G$, timestream $S$, and reference timestream $R$ is:<br />$S = G R$<br /><br />Naively, one would assume that something like<br />$G = median(S/R)$<br />would work.  However, it doesn't.  The distribution of $G$ values looks like:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-A3u4lJpkgtM/Ta4jA5BIwiI/AAAAAAAAGH4/fRg_ExqMVHw/s1600/GainValues_Simple.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="194" width="320" src="http://4.bp.blogspot.com/-A3u4lJpkgtM/Ta4jA5BIwiI/AAAAAAAAGH4/fRg_ExqMVHw/s320/GainValues_Simple.png" /></a></div><br /><br /><br />Note that this is true <b> no matter what reference bolometer you use </b>.  I've monte-carlo tested this and shown that the ratio $S/R&lt;1$ for all bolometers.  This is actually a well-understood phenomenon if you look at, e.g., Wall and Jenkins section on linear least squares.  If both X and Y values are gaussian-distributed quantities, the linear-least-squares solution is not ideal.  Similarly, but more confusing to me, is that the min($\chi^2$) is not a correct solution either, probably because you're minimizing with respect to only one variable.<br /><br />The solution is Principal Component Analysis.  The most correlated component is pretty much the atmosphere (not exactly, but we'll deal with that later).  The important thing is that the first row of the eigenvectors gives the relative scaling of each timestream to that most correlated component.  Actually the meaning has to be more carefully defined than that, but I can't figure it out right now.  However, it looks like the relative scales come out as expected if you scale to a bolometer based on the first row (column?) of eigenvectors corresponding to the most correlated component.<br /><br />Using PCA and the correct scaling, the results are very pretty, i.e. perfect recovery:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-W7O1lULkZFo/Ta4qYfOn66I/AAAAAAAAGIA/eB-08uNfG-4/s1600/Gains_PCA.png" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="194" width="320" src="http://1.bp.blogspot.com/-W7O1lULkZFo/Ta4qYfOn66I/AAAAAAAAGIA/eB-08uNfG-4/s320/Gains_PCA.png" /></a></div><br />So... now on to the tests of everything ever...</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deconvolve and Epochs]]></title>
    <link href="http://keflavich.github.com/blog/2011/04/05/deconvolve-and-epochs/"/>
    <updated>2011-04-05T12:12:00-06:00</updated>
    <id>http://keflavich.github.com/blog/2011/04/05/deconvolve-and-epochs</id>
    <content type="html"><![CDATA[<div class='post'>
I've spent a large portion of the last week working on the deconvolver.  I found <a href="http://bolocam.blogspot.com/2011/03/workaround-for-individual-maps.html">previously</a> that a reconvolved map does a better job of restoring flux than the straight-up deconvolved map for point sources / pointing observations.<br /><br />However, the same update broke the regular mapping modes, leading to horrible instability in the mapping routines for large maps such as W5.  Curiously, it seems that the aspect that breaks is the weighting; somehow the noise drops precipitously in certain bolometers, leading to extremely high weights.  Perhaps they somehow dominate the PCA subtraction and therefore have all their noise removed?<br /><br />Either way, there are a few large-scale changes that need to be made:<br /><ol><li> Since Scaling and Weighting are now done on a whole-timestream basis, we should only map single epochs at once and coadd them after the fact.  This approach will also help relieve RAM strain.  Since it appears that individual observations are now reasonably convergent with the proper treatment of NANs in the deconvolution scheme, it should be possible to take any individual map and coadd it in a reasonable way.<br /><li> Bolometers with bad weights need to be thrown out.  Alternatively, and more appropriately, I need to discover WHY their weights are going bad.<br /></ol>We also need to explore different weighting schemes. <ol><li> 1/Variance over whole timestream (current default)<br /><li> 1/Variance on a per-scan basis (previous default) [based on PSDs]<br /><li> Minimum Chi<sup>2</sup> with Astrophysical Model (??)<br /><li> Min Chi<sup>2</sup> on a per-scan basis?<br /></ol><br />Because of the extensive testing this will require, it is really becoming essential that we develop an arbitrary map creation & testing routine.</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Weighting and high-frequency noise]]></title>
    <link href="http://keflavich.github.com/blog/2008/11/14/weighting-and-high-frequency-noise/"/>
    <updated>2008-11-14T16:02:00-07:00</updated>
    <id>http://keflavich.github.com/blog/2008/11/14/weighting-and-high-frequency-noise</id>
    <content type="html"><![CDATA[<div class='post'>
<a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="http://1.bp.blogspot.com/_lsgW26mWZnU/SR4DzVCz1WI/AAAAAAAADfk/cJ70OPeqEzg/s1600-h/psds.png"><img style="cursor: pointer; width: 400px; height: 237px;" src="http://1.bp.blogspot.com/_lsgW26mWZnU/SR4DzVCz1WI/AAAAAAAADfk/cJ70OPeqEzg/s400/psds.png" alt="" id="BLOGGER_PHOTO_ID_5268652794427200866" border="0" /></a><br /><br />Image of PSDs (with no normalization) of the raw (blue), delined and exponential and polynomial subtracted (white), the noise timestream (yellow), and the data (cyan).<br /><br />The good: It looks like all of the powerline noise got into the noise timestream and almost none in the data.<br /><br />The bad: weighting is based on the noise timestream so it's possible that the weights aren't quite right as a result<br /><br />The weird: the data PSD.  What's up with that?  Apparently I'm preferentially subtracting certain scales but I don't know why, unless deconvolution is at fault.<br /><br /><br />Edit/Update: The deconvolution is definitely at fault.  Here's the same scan done without deconvolution:<br /><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="http://3.bp.blogspot.com/_lsgW26mWZnU/SR4ZM6FPfrI/AAAAAAAADf0/baHOQwedeqs/s1600-h/psds2.png"><img style="cursor:pointer; cursor:hand;width: 400px; height: 239px;" src="http://3.bp.blogspot.com/_lsgW26mWZnU/SR4ZM6FPfrI/AAAAAAAADf0/baHOQwedeqs/s400/psds2.png" border="0" alt=""id="BLOGGER_PHOTO_ID_5268676323610427058" /></a><br /><br />It should have been obvious; the cyan in the first plot is the PSD of the deconvolution straight up, and that should have no high-frequency structure...</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Implementing weighting]]></title>
    <link href="http://keflavich.github.com/blog/2008/11/08/implementing-weighting/"/>
    <updated>2008-11-08T21:46:00-07:00</updated>
    <id>http://keflavich.github.com/blog/2008/11/08/implementing-weighting</id>
    <content type="html"><![CDATA[<div class='post'>
Not as easy as it ought to be.<br /><br /><br />I think I need to do a few things:<br />1. check and make sure there are no more of those !@#$!@#$#@% different sized array subtractions/multiplications.  'weight' and 'best_astro_model' need to have the same size & shape in mem_iter_pc<br />2. I guess just check and make sure stuff works.  The weighted mean I'm using appears to be right: sum(weight * value) / sum(weight)<br /><br />I hate making lists that end up being two items....</div>

]]></content>
  </entry>
  
</feed>
